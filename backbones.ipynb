{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e286c2c",
   "metadata": {},
   "source": [
    "**To calculate computational efficieny on differencnt backbones**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5412ed8d",
   "metadata": {},
   "source": [
    "**Load Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "502d3f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/.conda/envs/pyenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from models.DiGATe_Unet import DiGATe_Unet\n",
    "\n",
    "def load_digat_model(ex_no: str, backbone: str = \"tf_efficientnet_b4\", base_dir: str = \"weights\", device: str | None = None) -> torch.nn.Module:\n",
    "\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model = DiGATe_Unet(\n",
    "        n_classes=1,\n",
    "        backbone=backbone,\n",
    "        n_channels=3,\n",
    "        pretrained=True,\n",
    "        pretrained_path=None,\n",
    "        use_input_adapter=False,\n",
    "        freeze_backbone=True,\n",
    "        share_backbone=False,\n",
    "    ).to(device)\n",
    "\n",
    "    checkpoint_path = os.path.join(base_dir, \"weights\", f\"{ex_no}.pth\")\n",
    "    checkpoint = torch.load(checkpoint_path, weights_only=False)\n",
    "\n",
    "    # Handle both full checkpoints and plain state-dicts\n",
    "    state_dict = (\n",
    "        checkpoint[\"model_state_dict\"]\n",
    "        if \"model_state_dict\" in checkpoint\n",
    "        else checkpoint\n",
    "    )\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794029e9",
   "metadata": {},
   "source": [
    "**Computational Cost Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82fce821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from thop import profile\n",
    "import copy\n",
    "\n",
    "def analyze_model_efficiency(model, inputs):\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = tuple(i.to(device) for i in inputs)\n",
    "    model.eval()\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # 1. GFLOPs and Total Parameters\n",
    "    model_for_profile = copy.deepcopy(model)\n",
    "    with torch.no_grad():\n",
    "        flops, params = profile(model_for_profile, inputs=inputs, verbose=False)\n",
    "        results['GFLOPs'] = flops / 1e9\n",
    "        results['Total Params (M)'] = params / 1e6\n",
    "\n",
    "    # 2. Trainable Parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    results['Trainable Params (M)'] = trainable_params / 1e6\n",
    "\n",
    "    # 3. FPS, Latency, and Memory (GPU-specific)\n",
    "    if device.type == 'cuda':\n",
    "        # Memory Usage\n",
    "        torch.cuda.reset_peak_memory_stats(device)\n",
    "        with torch.no_grad():\n",
    "            _ = model(*inputs)\n",
    "        peak_memory_mb = torch.cuda.max_memory_allocated(device) / (1024 * 1024)\n",
    "        results['Peak Memory (MB)'] = peak_memory_mb\n",
    "\n",
    "        # FPS and Latency\n",
    "        num_warmup = 20\n",
    "        num_runs = 100\n",
    "        \n",
    "        # Create CUDA events for accurate timing\n",
    "        start_event = torch.cuda.Event(enable_timing=True)\n",
    "        end_event = torch.cuda.Event(enable_timing=True)\n",
    "        \n",
    "        # Warm-up runs\n",
    "        with torch.no_grad():\n",
    "            for _ in range(num_warmup):\n",
    "                _ = model(*inputs)\n",
    "        \n",
    "        # Timing runs\n",
    "        times = []\n",
    "        with torch.no_grad():\n",
    "            for _ in range(num_runs):\n",
    "                start_event.record()\n",
    "                _ = model(*inputs)\n",
    "                end_event.record()\n",
    "                \n",
    "                # Wait for events to complete\n",
    "                torch.cuda.synchronize()\n",
    "                \n",
    "                times.append(start_event.elapsed_time(end_event)) # Time in ms\n",
    "        \n",
    "        avg_latency_ms = sum(times) / len(times)\n",
    "        fps = 1000.0 / avg_latency_ms\n",
    "        \n",
    "        results['Latency (ms)'] = avg_latency_ms\n",
    "        results['FPS'] = fps\n",
    "\n",
    "    else: # CPU measurements\n",
    "        # Note: CPU timing is less precise and memory is harder to isolate.\n",
    "        num_warmup = 10\n",
    "        num_runs = 50\n",
    "        \n",
    "        # Warm-up\n",
    "        with torch.no_grad():\n",
    "            for _ in range(num_warmup):\n",
    "                _ = model(*inputs)\n",
    "\n",
    "        # Timing\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(num_runs):\n",
    "                _ = model(*inputs)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        avg_latency_s = (end_time - start_time) / num_runs\n",
    "        results['Latency (ms)'] = avg_latency_s * 1000\n",
    "        results['FPS'] = 1.0 / avg_latency_s\n",
    "        results['Peak Memory (MB)'] = 'N/A on CPU'\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3183657",
   "metadata": {},
   "source": [
    "**Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f077806",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"/home/user1/ms/DiGATe-UNet-LandSlide-Segmentation\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdc49d9",
   "metadata": {},
   "source": [
    "**Computational Cost Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9aa265c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected keys (bn2.bias, bn2.num_batches_tracked, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n",
      "Unexpected keys (bn2.bias, bn2.num_batches_tracked, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model Efficiency Report ---\n",
      "-----------------------------\n",
      "GFLOPs: 4.19\n",
      "Trainable Params (M): 1.238\n",
      "FPS: 40.34\n",
      "Peak Memory (MB): 303.62\n"
     ]
    }
   ],
   "source": [
    "model = load_digat_model(\"E02\", backbone=\"tf_efficientnet_b4\", base_dir=BASE_DIR)\n",
    "\n",
    "input1 = torch.randn(1, 3, 256, 256, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input2 = torch.randn(1, 3, 256, 256, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "inputs = (input1, input2)\n",
    "\n",
    "# Get the efficiency report\n",
    "efficiency_report = analyze_model_efficiency(model, inputs)\n",
    "\n",
    "print(f\"--- Model Efficiency Report ---\")\n",
    "print(\"-----------------------------\")\n",
    "print(f\"GFLOPs: {efficiency_report.get('GFLOPs', 0):.2f}\")\n",
    "print(f\"Trainable Params (M): {efficiency_report.get('Trainable Params (M)', 0):.3f}\")\n",
    "print(f\"FPS: {efficiency_report.get('FPS', 0):.2f}\")\n",
    "print(f\"Peak Memory (MB): {efficiency_report.get('Peak Memory (MB)', 0):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca904c98",
   "metadata": {},
   "source": [
    "**resnet50**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71582e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model Efficiency Report ---\n",
      "-----------------------------\n",
      "GFLOPs: 22.38\n",
      "Trainable Params (M): 23.261\n",
      "FPS: 35.56\n",
      "Peak Memory (MB): 352.85\n"
     ]
    }
   ],
   "source": [
    "model = load_digat_model(\"d121\", backbone=\"densenet121\", base_dir=BASE_DIR)\n",
    "\n",
    "input1 = torch.randn(1, 3, 256, 256, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input2 = torch.randn(1, 3, 256, 256, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "inputs = (input1, input2)\n",
    "\n",
    "# Get the efficiency report\n",
    "efficiency_report = analyze_model_efficiency(model, inputs)\n",
    "\n",
    "print(f\"--- Model Efficiency Report ---\")\n",
    "print(\"-----------------------------\")\n",
    "print(f\"GFLOPs: {efficiency_report.get('GFLOPs', 0):.2f}\")\n",
    "print(f\"Trainable Params (M): {efficiency_report.get('Trainable Params (M)', 0):.3f}\")\n",
    "print(f\"FPS: {efficiency_report.get('FPS', 0):.2f}\")\n",
    "print(f\"Peak Memory (MB): {efficiency_report.get('Peak Memory (MB)', 0):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86541e4",
   "metadata": {},
   "source": [
    "**ViT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7701447a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user1/.conda/envs/pyenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/user1/.conda/envs/pyenv/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model Efficiency Report ---\n",
      "-----------------------------\n",
      "GFLOPs: 195.48\n",
      "Trainable Params (M): 39.101\n",
      "FPS: 22.41\n",
      "Peak Memory (MB): 2362.48\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from models.DiGATe_Unet_Vit import DiGATe_Unet_Vit\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {DEVICE}')\n",
    "\n",
    "model = DiGATe_Unet_Vit(1, backbone=\"vit_base_patch16_224\", model_path='/home/user1/.cache/torch/hub/checkpoints/vit_base_patch16_224.pth').to(DEVICE)\n",
    "checkpoint = torch.load(os.path.join(BASE_DIR, \"weights\", \"vit_224.pth\"), weights_only=False)\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "input1 = torch.randn(1, 3, 224, 224, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input2 = torch.randn(1, 3, 224, 224, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "inputs = (input1, input2)\n",
    "\n",
    "# Get the efficiency report\n",
    "efficiency_report = analyze_model_efficiency(model, inputs)\n",
    "\n",
    "print(f\"--- Model Efficiency Report ---\")\n",
    "print(\"-----------------------------\")\n",
    "print(f\"GFLOPs: {efficiency_report.get('GFLOPs', 0):.2f}\")\n",
    "print(f\"Trainable Params (M): {efficiency_report.get('Trainable Params (M)', 0):.3f}\")\n",
    "print(f\"FPS: {efficiency_report.get('FPS', 0):.2f}\")\n",
    "print(f\"Peak Memory (MB): {efficiency_report.get('Peak Memory (MB)', 0):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pyenv)",
   "language": "python",
   "name": "pyenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
